
# Resultados â€“ Tarea 3: ComunicaciÃ³n Colectiva y MediciÃ³n de Latencia

**Curso:** ComputaciÃ³n Paralela  
**Profesor:** Johansell Villalobos Cubillo  
**Estudiante:** [Tu nombre]  
**Fecha:** [Fecha actual]

---

## ğŸ§© Parte A â€“ Operaciones Colectivas con MPI

### ğŸ–¥ï¸ EjecuciÃ³n

```bash
mpiexec -n 4 python MPI_stats.py 1000000
```

### ğŸ“¸ Evidencia de ejecuciÃ³n

![Resultados Parte A](ecdd1d55-2183-4167-bfa6-125a10a506fb.png)

### ğŸ“Š AnÃ¡lisis

- El mÃ­nimo fue 0.00 y el mÃ¡ximo 100.00, como se espera en una distribuciÃ³n uniforme [0, 100].
- El promedio de 50.02 tambiÃ©n confirma la correcta agregaciÃ³n global con `MPI_Reduce`.

---

## â± Parte B â€“ MediciÃ³n de Latencia Punto a Punto

### ğŸ–¥ï¸ EjecuciÃ³n

```bash
mpiexec -n 2 python latency_MPI.py 1000000
```

### ğŸ“¸ Evidencia de ejecuciÃ³n

![Resultados Parte B](d0d02136-6518-4248-8cb3-8d8435376d94.png)

### ğŸ“Š AnÃ¡lisis

- La latencia de ida y vuelta fue de 2.05 Âµs, estimando una latencia de ida de 1.02 Âµs.
- Esto indica una comunicaciÃ³n extremadamente rÃ¡pida en entorno local.

---

## ğŸ’¡ ReflexiÃ³n Final

- Las operaciones colectivas permiten simplificar el anÃ¡lisis paralelo.
- La latencia punto a punto es muy baja en entornos locales, pero su mediciÃ³n es clave para aplicaciones distribuidas.
- La implementaciÃ³n orientada a objetos con `logging` facilita la escalabilidad y depuraciÃ³n del cÃ³digo.
